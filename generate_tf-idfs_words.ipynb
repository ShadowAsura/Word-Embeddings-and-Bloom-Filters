{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18157 words with TF-IDFs\n",
      "Loaded 142049 tokenized sentences\n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import contextlib\n",
    "import string\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "import lemminflect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-computed TF-IDFs and tokenized corpus\n",
    "with open('data/fairytales_word_tf-idfs.json', 'r') as f:\n",
    "    tf_idfs = json.load(f)\n",
    "with open('data/fairytales_tokenized.json', 'r') as f:\n",
    "    tokenized_corpus = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(tf_idfs)} words with TF-IDFs\")\n",
    "print(f\"Loaded {len(tokenized_corpus)} tokenized sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip - data already loaded from JSON files above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip - functions not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip - corpus already processed and saved. Data loaded in cell 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/n_neighbors.json', 'r') as f:\n",
    "    n_neighbors = json.load(f)\n",
    "with open('data/neighbor_frequencies.json', 'r') as f:\n",
    "    neighbor_frequencies = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "tf_idfs = copy.deepcopy(neighbor_frequencies) # make copy of the relative frequencies json to convert frequencies to tf-idfs\n",
    "for word in neighbor_frequencies.keys(): # ex: \"queen\"\n",
    "    for neighbor in neighbor_frequencies[word].keys(): # The words next to \"queen\", i.e. \"lady\", \"fairy\", \"king\", etc.\n",
    "        tf_idfs[word][neighbor] = math.log(neighbor_frequencies[word][neighbor] / n_neighbors[word] * math.log(words_in_corpus / n_neighbors[neighbor]))\n",
    "\n",
    "for word in tf_idfs.keys():\n",
    "    tf_idfs[word] = dict(sorted(tf_idfs[word].items(), key=lambda item: item[1], reverse=True)) # sorts TF-IDFs in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m dist = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtf_idfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m df = pd.Series(dist)\n\u001b[32m      5\u001b[39m Q1 = df.quantile(\u001b[32m0.25\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dist = np.concatenate([list(x.values()) for x in tf_idfs.values()])\n",
    "df = pd.Series(dist)\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "(df > (Q3 + 1.5 * IQR)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2.871242e+06\n",
       "mean    -4.928283e+00\n",
       "std      1.728402e+00\n",
       "min     -9.715406e+00\n",
       "25%     -6.144078e+00\n",
       "50%     -5.009849e+00\n",
       "75%     -3.843962e+00\n",
       "max      2.597717e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum = df.min()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in tf_idfs.keys():\n",
    "    for neighbor in tf_idfs[word].keys():\n",
    "        tf_idfs[word][neighbor] -= minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/fairytales_word_tf-idfs.json', 'w') as f:\n",
    "    json.dump(tf_idfs, f, indent=4)\n",
    "with open('data/fairytales_tokenized.json', 'w') as f:\n",
    "    json.dump(tokenized_corpus, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_tf_idf_distribution(tf_idfs, title, bounds=(0,1)):\n",
    "    all_values = []\n",
    "    for word in tf_idfs.keys():\n",
    "        all_values.extend(tf_idfs[word].values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(all_values, bins=50, range=bounds, edgecolor='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('TF-IDF')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_tf_idf_distribution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_tf_idf_distribution\u001b[49m(tf_idfs, \u001b[33m'\u001b[39m\u001b[33mTF-IDFs\u001b[39m\u001b[33m'\u001b[39m, bounds=(\u001b[32m0\u001b[39m, \u001b[32m13\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_tf_idf_distribution' is not defined"
     ]
    }
   ],
   "source": [
    "plot_tf_idf_distribution(tf_idfs, 'TF-IDFs', bounds=(0, 13))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
